{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3deb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient Methods for Visualizing the Sparse Term-Document Matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== Method 1: Sparsity Pattern Visualization ===\")\n",
    "# Show the structure of sparsity in a subset of the matrix\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Take a sample for visualization (first 100 documents, first 200 features)\n",
    "sample_matrix = X_counts[:100, :200].toarray()\n",
    "\n",
    "# Method 1a: Spy plot to show non-zero entries\n",
    "ax1.spy(sample_matrix, markersize=0.5)\n",
    "ax1.set_title('Sparsity Pattern (100 docs × 200 terms)')\n",
    "ax1.set_xlabel('Term Index')\n",
    "ax1.set_ylabel('Document Index')\n",
    "\n",
    "# Method 1b: Heatmap of most frequent terms\n",
    "# Get the most frequent terms across all documents\n",
    "term_sums = np.array(X_counts.sum(axis=0)).flatten()\n",
    "top_term_indices = np.argsort(term_sums)[-50:]  # Top 50 most frequent terms\n",
    "feature_names = count_vect.get_feature_names_out()\n",
    "top_terms = feature_names[top_term_indices]\n",
    "\n",
    "# Create heatmap with top terms and sample documents\n",
    "heatmap_data = X_counts[:20, top_term_indices].toarray()\n",
    "sns.heatmap(heatmap_data, \n",
    "            xticklabels=top_terms,\n",
    "            yticklabels=[f'Doc {i}' for i in range(20)],\n",
    "            cmap='YlOrRd',\n",
    "            ax=ax2,\n",
    "            cbar_kws={'label': 'Term Frequency'})\n",
    "ax2.set_title('Heatmap: Top 50 Terms × 20 Documents')\n",
    "ax2.set_xlabel('Terms')\n",
    "ax2.set_ylabel('Documents')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Matrix sparsity: {(X_counts.nnz / (X_counts.shape[0] * X_counts.shape[1]) * 100):.2f}% non-zero elements\")\n",
    "print(f\"Total vocabulary size: {len(feature_names)}\")\n",
    "print(f\"Total documents: {X_counts.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Method 2: Dimensionality Reduction Visualization ===\")\n",
    "# Use PCA to reduce dimensionality and visualize document relationships\n",
    "\n",
    "# Apply PCA to reduce to 2D\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_counts.toarray())\n",
    "\n",
    "# Create a scatter plot colored by newsgroup category\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(twenty_train.target_names)))\n",
    "target_names = twenty_train.target_names\n",
    "\n",
    "for i, (color, target_name) in enumerate(zip(colors, target_names)):\n",
    "    mask = twenty_train.target == i\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                c=[color], label=target_name, alpha=0.7, s=30)\n",
    "\n",
    "plt.xlabel(f'First Principal Component (explains {pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'Second Principal Component (explains {pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('Document Clustering via PCA\\n(Each point is a document, colors represent newsgroup categories)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"PCA explains {pca.explained_variance_ratio_.sum():.1%} of total variance with 2 components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Method 3: Term Frequency Analysis ===\")\n",
    "# Analyze and visualize the most important terms\n",
    "\n",
    "# Calculate term frequencies across the entire corpus\n",
    "term_frequencies = np.array(X_counts.sum(axis=0)).flatten()\n",
    "feature_names = count_vect.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "term_freq_df = pd.DataFrame({\n",
    "    'term': feature_names,\n",
    "    'frequency': term_frequencies\n",
    "}).sort_values('frequency', ascending=False)\n",
    "\n",
    "# Plot the top 20 most frequent terms\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_20_terms = term_freq_df.head(20)\n",
    "plt.barh(range(len(top_20_terms)), top_20_terms['frequency'])\n",
    "plt.yticks(range(len(top_20_terms)), top_20_terms['term'])\n",
    "plt.xlabel('Total Frequency Across All Documents')\n",
    "plt.title('Top 20 Most Frequent Terms in the Corpus')\n",
    "plt.gca().invert_yaxis()  # To have the highest frequency at the top\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 most frequent terms:\")\n",
    "print(term_freq_df.head(10).to_string(index=False))\n",
    "print(f\"\\nTotal unique terms in vocabulary: {len(feature_names)}\")\n",
    "print(f\"Terms appearing only once: {sum(term_frequencies == 1)}\")\n",
    "print(f\"Terms appearing more than 100 times: {sum(term_frequencies > 100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Method 4: Document Length and Sparsity Analysis ===\")\n",
    "# Analyze document characteristics\n",
    "\n",
    "# Calculate document lengths (number of words per document)\n",
    "doc_lengths = np.array(X_counts.sum(axis=1)).flatten()\n",
    "doc_sparsity = np.array((X_counts > 0).sum(axis=1)).flatten()  # Number of unique terms per document\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Document length distribution\n",
    "ax1.hist(doc_lengths, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_xlabel('Document Length (Total Words)')\n",
    "ax1.set_ylabel('Number of Documents')\n",
    "ax1.set_title('Distribution of Document Lengths')\n",
    "ax1.axvline(np.mean(doc_lengths), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(doc_lengths):.1f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Unique terms per document\n",
    "ax2.hist(doc_sparsity, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "ax2.set_xlabel('Unique Terms per Document')\n",
    "ax2.set_ylabel('Number of Documents')\n",
    "ax2.set_title('Distribution of Vocabulary Diversity')\n",
    "ax2.axvline(np.mean(doc_sparsity), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(doc_sparsity):.1f}')\n",
    "ax2.legend()\n",
    "\n",
    "# Scatter plot: document length vs unique terms\n",
    "ax3.scatter(doc_lengths, doc_sparsity, alpha=0.6, s=20)\n",
    "ax3.set_xlabel('Document Length (Total Words)')\n",
    "ax3.set_ylabel('Unique Terms per Document')\n",
    "ax3.set_title('Document Length vs Vocabulary Diversity')\n",
    "\n",
    "# Add correlation coefficient\n",
    "correlation = np.corrcoef(doc_lengths, doc_sparsity)[0, 1]\n",
    "ax3.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "         transform=ax3.transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average document length: {np.mean(doc_lengths):.1f} words\")\n",
    "print(f\"Average unique terms per document: {np.mean(doc_sparsity):.1f}\")\n",
    "print(f\"Correlation between length and diversity: {correlation:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dm2025lab)",
   "language": "python",
   "name": "dm2025lab"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
